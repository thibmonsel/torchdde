{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"torchdde torchdde is a library that provides numerical solvers in Pytorch for Delay Differential Equations (DDEs) with constant delays. Warning This a brand new library, please reach out for feedback, issues ! Installation \u00a4 pip install git@github.com:thibmonsel/torchdde.git or locally git clone https://github.com/thibmonsel/torchdde.git pip install torchdde/ Documentation \u00a4 To generate the documentation locally, please look at CONTRIBUTING.MD . Quick example \u00a4 import torch from torchdde import integrate , RK2 def f ( t , y , args , history ): return y * ( 1 - history [ 0 ]) solver = RK2 () delays = torch . tensor ([ 1.0 ]) history_values = torch . arange ( 1 , 5 ) . reshape ( - 1 , 1 ) history_function = lambda t : history_values solution = integrate ( f , solver , ts [ 0 ], ts [ - 1 ], ts , y0 , None , dt0 = ts [ 1 ] - ts [ 0 ], delays = delays )","title":"Home"},{"location":"#installation","text":"pip install git@github.com:thibmonsel/torchdde.git or locally git clone https://github.com/thibmonsel/torchdde.git pip install torchdde/","title":"Installation"},{"location":"#documentation","text":"To generate the documentation locally, please look at CONTRIBUTING.MD .","title":"Documentation"},{"location":"#quick-example","text":"import torch from torchdde import integrate , RK2 def f ( t , y , args , history ): return y * ( 1 - history [ 0 ]) solver = RK2 () delays = torch . tensor ([ 1.0 ]) history_values = torch . arange ( 1 , 5 ) . reshape ( - 1 , 1 ) history_function = lambda t : history_values solution = integrate ( f , solver , ts [ 0 ], ts [ - 1 ], ts , y0 , None , dt0 = ts [ 1 ] - ts [ 0 ], delays = delays )","title":"Quick example"},{"location":"interpolators/linear-interpolator/","text":"This library only supports linear interpolation at this moment ! torchdde.TorchLinearInterpolator \u00a4 Linear interpolator class that is compatible with batching. All elements in batch must have the same ts. __init__ ( self , ts : Float [ Tensor , 'time' ], ys : Float [ Tensor , 'batch time ...' ]) \u00a4 Arguments: ts : Some increasing collection of times. ys : The observations themselves. __call__ ( self , t : Union [ Float [ Tensor , '' ], float ], left = True ) -> Float [ Tensor , 'batch ...' ] \u00a4 Arguments: t : time to evalute to add_point ( self , new_t : Float [ Tensor , '' ], new_y : Float [ Tensor , 'batch ...' ]) -> None \u00a4 Arguments: new_t : new timestamp added to the interpolation new_ys : new observation added to the interpolation","title":"Linear interpolator"},{"location":"interpolators/linear-interpolator/#torchdde.TorchLinearInterpolator","text":"Linear interpolator class that is compatible with batching. All elements in batch must have the same ts.","title":"TorchLinearInterpolator"},{"location":"usage/contribute/","text":"If you wish to contribute there would be plenty to do for example : Encapsulate all Runge Kutta methods (e.g. RK2(),RK4(),Dopri5()... ) in one abstract class AbstractRK . One example of this is in Diffrax or in torchode . Attend this exploration phase for the adjoint method from the issue .","title":"Contribute"},{"location":"usage/faq/","text":"FAQ \u00a4 How would I define a DDE with several delays ? \u00a4 You just have to specify a delay tensor size that corresponds to the number of delays you desire. solver = .... delays = torch . tensor ([ 1.0 , 2.0 ]) history_function = lambda t : ... ts = ... def simple_dde ( t , y , args , * , history ): # this correspond to y'(t) = -y(t-1) - y(t-2) return - history [ 0 ] - history [ 1 ] ys = torchdde . integrate ( f , solver , ts [ 0 ], ts [ - 1 ], ts , history_function , args = None , dt0 = ts [ 1 ] - ts [ 0 ], delays = delays ) How about if I want a neural network to have also several delays ? \u00a4 Well if its the same forward pass in the Neural DDE , then nothing needs to be changed ! The term torch.cat([z, *history], dim=-1) unpacks all the delayed terms.","title":"FAQ"},{"location":"usage/faq/#faq","text":"","title":"FAQ"},{"location":"usage/faq/#how-would-i-define-a-dde-with-several-delays","text":"You just have to specify a delay tensor size that corresponds to the number of delays you desire. solver = .... delays = torch . tensor ([ 1.0 , 2.0 ]) history_function = lambda t : ... ts = ... def simple_dde ( t , y , args , * , history ): # this correspond to y'(t) = -y(t-1) - y(t-2) return - history [ 0 ] - history [ 1 ] ys = torchdde . integrate ( f , solver , ts [ 0 ], ts [ - 1 ], ts , history_function , args = None , dt0 = ts [ 1 ] - ts [ 0 ], delays = delays )","title":"How would I define a DDE with several delays ?"},{"location":"usage/faq/#how-about-if-i-want-a-neural-network-to-have-also-several-delays","text":"Well if its the same forward pass in the Neural DDE , then nothing needs to be changed ! The term torch.cat([z, *history], dim=-1) unpacks all the delayed terms.","title":"How about if I want a neural network to have also several delays ?"},{"location":"usage/getting-started/","text":"Getting Started \u00a4 Integrating ... \u00a4 ... DDEs \u00a4 We provide an illustrative example which solves the following DDE : \\(\\frac{dy}{dt}= -y(t-2), \\quad \\psi(t<0) = 2\\) over the interval \\([0, 5]\\) . import matplotlib.pyplot as plt from torchdde import RK4 import torch def simple_dde ( t , y , args , * , history ): return - history [ 0 ] device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) # Define the time span of the DDE ts = torch . linspace ( 0 , 5 , 51 ) ts = ts . to ( device ) # Define the delays, here there is only one tau=2.0 delays = torch . tensor ([ 2.0 ]) delays = delays . to ( device ) # Defining a constant history function for the DDE history_values = torch . tensor ([ 3.0 ]) history_values = history_values . reshape ( - 1 , 1 ) history_values = history_values . to ( device ) history_function = lambda t : history_values # Solve the DDE by using the RK4 method solver = RK4 () solution = torchdde . integrate ( simple_dde , solver , ts [ 0 ], ts [ - 1 ], ts , history_function , None , dt0 = ts [ 1 ] - ts [ 0 ], delays = delays ) The numerical solver used is RK4 is the 4th Runge Kutta method. The solution is saved at each time stamp in ts . The initial step size is equal to ts[1]-ts[0] . ... ODEs \u00a4 We provide an illustrative example which solves the following ODE : \\(\\frac{dy}{dt}= -y(t)^{2}, \\quad y(0) = 2\\) over the interval \\([0, 5]\\) . import matplotlib.pyplot as plt from torchdde import RK4 import torch def simple_ode ( t , y , args ): return - y ** 2 device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) # Define the time span of the ODE ts = torch . linspace ( 0 , 5 , 51 ) ts = ts . to ( device ) # Initial condition y0 = torch . tensor ([ 3.0 ]) y0 = y0 . reshape ( - 1 , 1 ) y0 = y0 . to ( device ) # Solve the ODE by using the RK4 method solver = RK4 () solution = torchdde . integrate ( simple_ode , solver , ts [ 0 ], ts [ - 1 ], ts , y0 , None , dt0 = ts [ 1 ] - ts [ 0 ], delays = None ) The numerical solver used is RK4 is the 4th Runge Kutta method. The solution is saved at each time stamp in ts . The initial step size is equal to ts[1]-ts[0] .","title":"Getting Started"},{"location":"usage/getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"usage/getting-started/#integrating","text":"","title":"Integrating ..."},{"location":"usage/getting-started/#ddes","text":"We provide an illustrative example which solves the following DDE : \\(\\frac{dy}{dt}= -y(t-2), \\quad \\psi(t<0) = 2\\) over the interval \\([0, 5]\\) . import matplotlib.pyplot as plt from torchdde import RK4 import torch def simple_dde ( t , y , args , * , history ): return - history [ 0 ] device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) # Define the time span of the DDE ts = torch . linspace ( 0 , 5 , 51 ) ts = ts . to ( device ) # Define the delays, here there is only one tau=2.0 delays = torch . tensor ([ 2.0 ]) delays = delays . to ( device ) # Defining a constant history function for the DDE history_values = torch . tensor ([ 3.0 ]) history_values = history_values . reshape ( - 1 , 1 ) history_values = history_values . to ( device ) history_function = lambda t : history_values # Solve the DDE by using the RK4 method solver = RK4 () solution = torchdde . integrate ( simple_dde , solver , ts [ 0 ], ts [ - 1 ], ts , history_function , None , dt0 = ts [ 1 ] - ts [ 0 ], delays = delays ) The numerical solver used is RK4 is the 4th Runge Kutta method. The solution is saved at each time stamp in ts . The initial step size is equal to ts[1]-ts[0] .","title":"... DDEs"},{"location":"usage/getting-started/#odes","text":"We provide an illustrative example which solves the following ODE : \\(\\frac{dy}{dt}= -y(t)^{2}, \\quad y(0) = 2\\) over the interval \\([0, 5]\\) . import matplotlib.pyplot as plt from torchdde import RK4 import torch def simple_ode ( t , y , args ): return - y ** 2 device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) # Define the time span of the ODE ts = torch . linspace ( 0 , 5 , 51 ) ts = ts . to ( device ) # Initial condition y0 = torch . tensor ([ 3.0 ]) y0 = y0 . reshape ( - 1 , 1 ) y0 = y0 . to ( device ) # Solve the ODE by using the RK4 method solver = RK4 () solution = torchdde . integrate ( simple_ode , solver , ts [ 0 ], ts [ - 1 ], ts , y0 , None , dt0 = ts [ 1 ] - ts [ 0 ], delays = None ) The numerical solver used is RK4 is the 4th Runge Kutta method. The solution is saved at each time stamp in ts . The initial step size is equal to ts[1]-ts[0] .","title":"... ODEs"},{"location":"usage/integration-de/","text":"Integration \u00a4 First, there are a lot of available package to use to train/integrate Neural ODEs, torchdiffeq (not maintained anymore) in Pytorch and Diffrax (which is the gold standard here). This means that this library doesn't have any many features since it focuses more on DDEs. Regarless, the only entry point to integrate ODEs/DDEs is the integrate function. What essentially differentiates DDEs with ODEs are : the vector field definition. the delays argument specification. What changes in an ODE compared to a DDE ? In practice, your function will be defined like this : def f_ode ( t , y , args ): return ... def f_dde ( t , y , args , history ): return ... and your ODE's initial condition y0 will become a history function history_fn = lambda t : ... torchdde . integrate . integrate ( func : Union [ torch . nn . modules . module . Module , Callable ], solver : AbstractOdeSolver , t0 : Float [ Tensor , '' ], t1 : Float [ Tensor , '' ], ts : Float [ Tensor , 'time' ], y0 : Union [ Float [ Tensor , 'batch ...' ], Callable [[ Float [ Tensor , '' ]], Float [ Tensor , 'batch ...' ]]], args : Any , stepsize_controller : AbstractStepSizeController = < torchdde . step_size_controller . constant . ConstantStepSizeController object at 0x7fb5a3fb62d0 > , dt0 : Optional [ Float [ Tensor , '' ]] = None , delays : Optional [ Float [ Tensor , 'delays' ]] = None , discretize_then_optimize : bool = False , max_steps : int = 2048 ) -> Float [ Tensor , 'batch time ...' ] \u00a4 Solves a system of ODEs or DDEs. See the Getting started page for example usage. Main arguments: These are the arguments most commonly used day-to-day. func : The terms of the differential equation. This specifies the vector field. solver : The solver for the differential equation. See the available solvers . t0 : The start of the region of integration. t1 : The end of the region of integration. ts : The time points at which to return the solution. y0 : The initial value. This is either a tensor (for ODEs) or a history function callable (for DDEs) args : Any additional arguments to pass to the vector field. stepsize_controller : How to change the step size as the integration progresses. See the list of stepsize controllers . Defaults set to ConstantStepSizeController . dt0 : The step size to use for the first step. If you are using a fixed step sizes then this will also be the step size for all other steps. If set as None then the initial step size will be determined automatically, only available for torchdde.AdaptiveStepSizeController . delays : The initial values given to the constant delays for the DDE. If set to None then the system is treated as an ODE. discretize_then_optimize : If set to False , the adjoint method will be used for training. If set to True , regular backpropagation will be used for training. max_steps : The maximum number of steps to take before quitting the computation unconditionally. Returns: Returns the solution of the differential equation.","title":"Integration"},{"location":"usage/integration-de/#integration","text":"First, there are a lot of available package to use to train/integrate Neural ODEs, torchdiffeq (not maintained anymore) in Pytorch and Diffrax (which is the gold standard here). This means that this library doesn't have any many features since it focuses more on DDEs. Regarless, the only entry point to integrate ODEs/DDEs is the integrate function. What essentially differentiates DDEs with ODEs are : the vector field definition. the delays argument specification. What changes in an ODE compared to a DDE ? In practice, your function will be defined like this : def f_ode ( t , y , args ): return ... def f_dde ( t , y , args , history ): return ... and your ODE's initial condition y0 will become a history function history_fn = lambda t : ...","title":"Integration"},{"location":"usage/integration-de/#torchdde.integrate.integrate","text":"Solves a system of ODEs or DDEs. See the Getting started page for example usage. Main arguments: These are the arguments most commonly used day-to-day. func : The terms of the differential equation. This specifies the vector field. solver : The solver for the differential equation. See the available solvers . t0 : The start of the region of integration. t1 : The end of the region of integration. ts : The time points at which to return the solution. y0 : The initial value. This is either a tensor (for ODEs) or a history function callable (for DDEs) args : Any additional arguments to pass to the vector field. stepsize_controller : How to change the step size as the integration progresses. See the list of stepsize controllers . Defaults set to ConstantStepSizeController . dt0 : The step size to use for the first step. If you are using a fixed step sizes then this will also be the step size for all other steps. If set as None then the initial step size will be determined automatically, only available for torchdde.AdaptiveStepSizeController . delays : The initial values given to the constant delays for the DDE. If set to None then the system is treated as an ODE. discretize_then_optimize : If set to False , the adjoint method will be used for training. If set to True , regular backpropagation will be used for training. max_steps : The maximum number of steps to take before quitting the computation unconditionally. Returns: Returns the solution of the differential equation.","title":"integrate()"},{"location":"usage/neural-dde/","text":"Neural DDE \u00a4 Warning This library only supports constant lag DDEs. Therefore we are unable to model time and state dependent DDEs. This examples trains a Neural DDE to reproduce a simple dataset of a delay logistic equation. In this example, the backward pass is computed with the adjoint method. import time import matplotlib.pyplot as plt import torch import torch.nn as nn from torch.utils.data import DataLoader , Dataset from torchdde import integrate , Euler from torchvision.ops import MLP device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) Recalling that a Neural DDE is defined as \\[\\frac{dy}{dt} = f_{\\theta}(t, y(t), y(t-\\tau_1), \\dots, y(t-\\tau_{n})), \\quad y(t<0) = \\psi(t)\\] then here we're now about to define \\(f_{\\theta}\\) that appears on that right hand side on the equation above class NDDE ( nn . Module ): def __init__ ( self , delays , in_size , out_size , width_size , depth , ): super () . __init__ () self . in_dim = in_size * ( 1 + len ( delays )) self . delays = torch . nn . Parameter ( delays ) self . mlp = MLP ( self . in_dim , hidden_channels = depth * [ width_size ] + [ out_size ], ) def forward ( self , t , z , args , * , history ): return self . mlp ( torch . cat ([ z , * history ], dim =- 1 )) We generate the toy dataset of the delayed logistic equation (Equation 2.1). def get_data ( y0 , ts , tau = torch . tensor ([ 1.0 ])): def f ( t , y , args , history ): return y * ( 1 - history [ 0 ]) history_function = lambda t : torch . unsqueeze ( y0 , dim = 1 ) ys = integrate ( f , Euler (), ts [ 0 ], ts [ - 1 ], ts , history_function , args = None , dt0 = ts [ 1 ] - ts [ 0 ], delays = tau ) return ys class MyDataset ( Dataset ): def __init__ ( self , ys ): self . ys = ys def __getitem__ ( self , index ): return self . ys [ index ] def __len__ ( self ): return self . ys . shape [ 0 ] Main entry point. Try running main() . def main ( dataset_size = 128 , batch_size = 128 , lr = 0.0005 , max_epoch = 1000 , width_size = 32 , depth = 2 , seed = 5678 , plot = True , print_every = 5 , device = device , ): torch . manual_seed ( seed ) ts = torch . linspace ( 0 , 10 , 101 ) y0_min , y0_max = 2.0 , 3.0 y0 = ( y0_min - y0_max ) * torch . rand (( dataset_size ,)) + y0_max ys = get_data ( y0 , ts ) ts , ys = ts . to ( device ), ys . to ( device ) delay_min , delay_max = 0.7 , 1.3 value = ( delay_max - delay_min ) * torch . rand (( 1 ,)) + delay_min tau = torch . tensor ([ value ], device = device ) tau = tau . to ( device ) state_dim = ys . shape [ - 1 ] model = NDDE ( tau , state_dim , state_dim , width_size , depth ) model = model . to ( device ) dataset = MyDataset ( ys ) train_loader = DataLoader ( dataset , batch_size = batch_size , shuffle = True ) # Training loop like normal. model . train () loss_fn = torch . nn . MSELoss () optimizer = torch . optim . Adam ( model . parameters (), lr = lr ) for epoch in range ( max_epoch ): for step , data in enumerate ( train_loader ): t = time . time () optimizer . zero_grad () data = data . to ( device ) history_fn = lambda t : data [:, 0 ] ys_pred = integrate ( model , Euler (), ts [ 0 ], ts [ - 1 ], ts , history_fn , args = None , dt0 = ts [ 1 ] - ts [ 0 ], delays = tau ) loss = loss_fn ( ys_pred , data ) loss . backward () optimizer . step () if ( epoch % print_every ) == 0 or epoch == max_epoch - 1 : print ( \"Epoch : {} , Step {} / {} , Loss : {:.3e} , Tau {} , Time {} \" . format ( epoch , step + 1 , len ( train_loader ), loss . item (), [ d . item () for d in model . delays ], time . time () - t , ) ) if plot : plt . plot ( ts . cpu (), data [ 0 ] . cpu (), c = \"dodgerblue\" , label = \"Real\" ) history_values = data [ 0 , 0 ][ ... , None ] history_fn = lambda t : history_values ys_pred = integrate ( model , Euler (), ts [ 0 ], ts [ - 1 ], ts , history_fn , args = None , dt0 = ts [ 1 ] - ts [ 0 ], delays = tau ) plt . plot ( ts . cpu (), ys_pred [ 0 ] . cpu () . detach (), \"--\" , c = \"crimson\" , label = \"Model\" , ) plt . legend () plt . savefig ( \"neural_dde.png\" ) plt . show () plt . close () return ts , ys , model ts , ys , model = main ()","title":"Neural DDE"},{"location":"usage/neural-dde/#neural-dde","text":"Warning This library only supports constant lag DDEs. Therefore we are unable to model time and state dependent DDEs. This examples trains a Neural DDE to reproduce a simple dataset of a delay logistic equation. In this example, the backward pass is computed with the adjoint method. import time import matplotlib.pyplot as plt import torch import torch.nn as nn from torch.utils.data import DataLoader , Dataset from torchdde import integrate , Euler from torchvision.ops import MLP device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) Recalling that a Neural DDE is defined as \\[\\frac{dy}{dt} = f_{\\theta}(t, y(t), y(t-\\tau_1), \\dots, y(t-\\tau_{n})), \\quad y(t<0) = \\psi(t)\\] then here we're now about to define \\(f_{\\theta}\\) that appears on that right hand side on the equation above class NDDE ( nn . Module ): def __init__ ( self , delays , in_size , out_size , width_size , depth , ): super () . __init__ () self . in_dim = in_size * ( 1 + len ( delays )) self . delays = torch . nn . Parameter ( delays ) self . mlp = MLP ( self . in_dim , hidden_channels = depth * [ width_size ] + [ out_size ], ) def forward ( self , t , z , args , * , history ): return self . mlp ( torch . cat ([ z , * history ], dim =- 1 )) We generate the toy dataset of the delayed logistic equation (Equation 2.1). def get_data ( y0 , ts , tau = torch . tensor ([ 1.0 ])): def f ( t , y , args , history ): return y * ( 1 - history [ 0 ]) history_function = lambda t : torch . unsqueeze ( y0 , dim = 1 ) ys = integrate ( f , Euler (), ts [ 0 ], ts [ - 1 ], ts , history_function , args = None , dt0 = ts [ 1 ] - ts [ 0 ], delays = tau ) return ys class MyDataset ( Dataset ): def __init__ ( self , ys ): self . ys = ys def __getitem__ ( self , index ): return self . ys [ index ] def __len__ ( self ): return self . ys . shape [ 0 ] Main entry point. Try running main() . def main ( dataset_size = 128 , batch_size = 128 , lr = 0.0005 , max_epoch = 1000 , width_size = 32 , depth = 2 , seed = 5678 , plot = True , print_every = 5 , device = device , ): torch . manual_seed ( seed ) ts = torch . linspace ( 0 , 10 , 101 ) y0_min , y0_max = 2.0 , 3.0 y0 = ( y0_min - y0_max ) * torch . rand (( dataset_size ,)) + y0_max ys = get_data ( y0 , ts ) ts , ys = ts . to ( device ), ys . to ( device ) delay_min , delay_max = 0.7 , 1.3 value = ( delay_max - delay_min ) * torch . rand (( 1 ,)) + delay_min tau = torch . tensor ([ value ], device = device ) tau = tau . to ( device ) state_dim = ys . shape [ - 1 ] model = NDDE ( tau , state_dim , state_dim , width_size , depth ) model = model . to ( device ) dataset = MyDataset ( ys ) train_loader = DataLoader ( dataset , batch_size = batch_size , shuffle = True ) # Training loop like normal. model . train () loss_fn = torch . nn . MSELoss () optimizer = torch . optim . Adam ( model . parameters (), lr = lr ) for epoch in range ( max_epoch ): for step , data in enumerate ( train_loader ): t = time . time () optimizer . zero_grad () data = data . to ( device ) history_fn = lambda t : data [:, 0 ] ys_pred = integrate ( model , Euler (), ts [ 0 ], ts [ - 1 ], ts , history_fn , args = None , dt0 = ts [ 1 ] - ts [ 0 ], delays = tau ) loss = loss_fn ( ys_pred , data ) loss . backward () optimizer . step () if ( epoch % print_every ) == 0 or epoch == max_epoch - 1 : print ( \"Epoch : {} , Step {} / {} , Loss : {:.3e} , Tau {} , Time {} \" . format ( epoch , step + 1 , len ( train_loader ), loss . item (), [ d . item () for d in model . delays ], time . time () - t , ) ) if plot : plt . plot ( ts . cpu (), data [ 0 ] . cpu (), c = \"dodgerblue\" , label = \"Real\" ) history_values = data [ 0 , 0 ][ ... , None ] history_fn = lambda t : history_values ys_pred = integrate ( model , Euler (), ts [ 0 ], ts [ - 1 ], ts , history_fn , args = None , dt0 = ts [ 1 ] - ts [ 0 ], delays = tau ) plt . plot ( ts . cpu (), ys_pred [ 0 ] . cpu () . detach (), \"--\" , c = \"crimson\" , label = \"Model\" , ) plt . legend () plt . savefig ( \"neural_dde.png\" ) plt . show () plt . close () return ts , ys , model ts , ys , model = main ()","title":"Neural DDE"},{"location":"usage/solvers/","text":"Numerical Solvers \u00a4 ODE Solvers \u00a4 Only some explicit solvers are available to use but adding new ones is rather simple with torchdde.AbstractOdeSolver . torchdde.AbstractOdeSolver \u00a4 Base class for creating ODE solvers. All solvers should inherit from it. To create new solvers users must implement the init , step and order method. init ( self ) abstractmethod \u00a4 Initializes the solver. This method is called before the integration starts. order ( self ) -> int abstractmethod \u00a4 Returns the order of the solver. step ( self , func : Union [ torch . nn . modules . module . Module , Callable ], t : Float [ Tensor , '' ], y : Float [ Tensor , 'batch ...' ], dt : Float [ Tensor , '' ], args : Any , has_aux = False ) -> tuple [ Float [ Tensor , 'batch ...' ], Float [ Tensor , 'batch ...' ], dict [ str , Float [ Tensor , 'batch order' ]], Union [ Float [ Tensor , 'batch' ], Any ]] abstractmethod \u00a4 ODE's solver stepping method Arguments: func : Pytorch model or callable function, i.e vector field t : Current time step t y : Current state y dt : Step size dt has_aux : Whether the model/callable has an auxiliary output. has_aux ? A function with an auxiliary output can look like def f ( t , y , args ): return - y , ( \"Hello World\" , 1 ) The has_aux kwargs argument is used to compute the adjoint method Returns: The value of the solution at t+dt . A local error estimate made during the step. (Used by torchdde.AdaptiveStepSizeController controllers to change the step size.) It may be None for constant stepsize solvers for example. Dictionary that holds all the information needed to properly build the interpolation between t and t+dt . None if the model doesn't have an auxiliary output. build_interpolation ( self , t0 , t1 , dense_info ) -> Any abstractmethod \u00a4 Interpolator building method based on the solver's order. Arguments: t0 : The start of the interval over which the interpolation is defined. t1 : The end of the interval over which the interpolation is defined. dense_info : Dictionary that hold all the information needed to properly build the interpolation between t and t+dt . Returns: A Callable that can be used to interpolate the solution between t0 and t1 . Explicit Solvers \u00a4 torchdde.Euler ( AbstractOdeSolver ) \u00a4 Euler's method torchdde.RK2 ( AbstractOdeSolver ) \u00a4 2nd order explicit Runge-Kutta method torchdde.RK4 ( AbstractOdeSolver ) \u00a4 4th order explicit Runge-Kutta method torchdde.Dopri5 ( AbstractOdeSolver ) \u00a4 5th order order explicit Runge-Kutta method Dormand Prince Implicit Solvers \u00a4 torchdde.ImplicitEuler ( AbstractOdeSolver ) \u00a4 ImplicitEuler Euler's method","title":"Numerical Solvers"},{"location":"usage/solvers/#numerical-solvers","text":"","title":"Numerical Solvers"},{"location":"usage/solvers/#ode-solvers","text":"Only some explicit solvers are available to use but adding new ones is rather simple with torchdde.AbstractOdeSolver .","title":"ODE Solvers"},{"location":"usage/solvers/#torchdde.AbstractOdeSolver","text":"Base class for creating ODE solvers. All solvers should inherit from it. To create new solvers users must implement the init , step and order method.","title":"AbstractOdeSolver"},{"location":"usage/solvers/#explicit-solvers","text":"","title":"Explicit Solvers"},{"location":"usage/solvers/#torchdde.Euler","text":"Euler's method","title":"Euler"},{"location":"usage/solvers/#torchdde.RK2","text":"2nd order explicit Runge-Kutta method","title":"RK2"},{"location":"usage/solvers/#torchdde.RK4","text":"4th order explicit Runge-Kutta method","title":"RK4"},{"location":"usage/solvers/#torchdde.Dopri5","text":"5th order order explicit Runge-Kutta method Dormand Prince","title":"Dopri5"},{"location":"usage/solvers/#implicit-solvers","text":"","title":"Implicit Solvers"},{"location":"usage/solvers/#torchdde.ImplicitEuler","text":"ImplicitEuler Euler's method","title":"ImplicitEuler"},{"location":"usage/stepsize-controller/","text":"In order to adjust the time stepping during integrating we provide torchdde.AdaptiveStepSizeController for adaptive methods (like torchdde.Dopri5 ) and torchdde.ConstantStepSizeController for constant stepsive methods (like torchdde.RK4 ). torchdde.ConstantStepSizeController \u00a4 Constant step size controller that always returns the same step size. The user must define dt0 via torchdde.integrate.integrate torchdde.AdaptiveStepSizeController \u00a4 Adapts the step size to produce a solution accurate to a given tolerance. The tolerance is calculated as atol + rtol * y for the evolving solution y . Steps are adapted using a PID controller. Choosing tolerances The choice of rtol and atol are used to determine how accurately you would like the numerical approximation to your equation. If you are solving a problem \"harder\" problem then you probably need to raise the tolerances to get an appropriate solution. Default values usually are rtol=1e-3 and atol=1e-6 . Choosing PID coefficients We refer the reader to Diffrax clear explanation here . __init__ ( self , atol , rtol , safety = 0.9 , pcoeff = 0.0 , icoeff = 1.0 , dcoeff = 0.0 , factormax = 10.0 , dtmin = None , dtmax = None ) \u00a4 Arguments: atol : Absolute tolerance. rtol : Relative tolerance. safety : Multiplicative safety factor. pcoeff : The coefficient of the proportional part of the step size control. icoeff : The coefficient of the integral part of the step size control. dcoeff : The coefficient of the derivative part of the step size control. factormax : Maximum amount a step size can be increased relative to the previous step. dtmin : Minimum step size. The step size is either clipped to this value, or an error raised if the step size decreases below this, depending on force_dtmin . dtmax : Maximum step size; the step size is clipped to this value.","title":"Stepsize controller"},{"location":"usage/stepsize-controller/#torchdde.ConstantStepSizeController","text":"Constant step size controller that always returns the same step size. The user must define dt0 via torchdde.integrate.integrate","title":"ConstantStepSizeController"},{"location":"usage/stepsize-controller/#torchdde.AdaptiveStepSizeController","text":"Adapts the step size to produce a solution accurate to a given tolerance. The tolerance is calculated as atol + rtol * y for the evolving solution y . Steps are adapted using a PID controller. Choosing tolerances The choice of rtol and atol are used to determine how accurately you would like the numerical approximation to your equation. If you are solving a problem \"harder\" problem then you probably need to raise the tolerances to get an appropriate solution. Default values usually are rtol=1e-3 and atol=1e-6 . Choosing PID coefficients We refer the reader to Diffrax clear explanation here .","title":"AdaptiveStepSizeController"},{"location":"usage/training-de/","text":"Training \u00a4 Two following ways are possible to train Neural DDEs / Neural ODEs : optimize-then-discretize (with the adjoint method) discretize-then-optimize (regular backpropagation) Please see the doctorial thesis On Neural Differential Equations for more information on both procedures. To choose from either two possibilities is easy, you just need to set the bool discretize_then_optimize . A simple training loop would look like: for step , data in enumerate ( train_loader ): optimizer . zero_grad () data = data . to ( device ) ys_pred = integrate ( model , solver =... , t0 = ts [ 0 ], t1 = ts [ - 1 ], ts = ts , y0 =... , args = None , dt0 = ts [ 1 ] - ts [ 0 ], delays =... , ) loss = loss_fn ( ys_pred , data ) loss . backward () optimizer . step () If you are training an ODE then delays=None and y0 is a Tensor . If you are training an DDE then delays is a Tensor and y0 is a Callable .","title":"Training"},{"location":"usage/training-de/#training","text":"Two following ways are possible to train Neural DDEs / Neural ODEs : optimize-then-discretize (with the adjoint method) discretize-then-optimize (regular backpropagation) Please see the doctorial thesis On Neural Differential Equations for more information on both procedures. To choose from either two possibilities is easy, you just need to set the bool discretize_then_optimize . A simple training loop would look like: for step , data in enumerate ( train_loader ): optimizer . zero_grad () data = data . to ( device ) ys_pred = integrate ( model , solver =... , t0 = ts [ 0 ], t1 = ts [ - 1 ], ts = ts , y0 =... , args = None , dt0 = ts [ 1 ] - ts [ 0 ], delays =... , ) loss = loss_fn ( ys_pred , data ) loss . backward () optimizer . step () If you are training an ODE then delays=None and y0 is a Tensor . If you are training an DDE then delays is a Tensor and y0 is a Callable .","title":"Training"}]}