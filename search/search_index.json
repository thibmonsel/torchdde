{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"torchdde \u00a4 torchdde is a library providing Constant Lag Delay Differential Equations (DDEs) training neural networks via the adjoint method compatible with Pytorch . Installation \u00a4 pip install git@github.com:usr/torchdde.git or locally git clone https://github.com/usr/torchdde.git cd torchdde/ pip install . Quick example \u00a4 def f ( t , y , args , history ): return y * ( 1 - history [ 0 ]) delays = torch . tensor ([ 1.0 ]) solver = DDESolver ( RK2 (), delays ) history_values = torch . arange ( 1 , 5 ) . reshape ( - 1 , 1 ) history_function = lambda t : history_values solution , _ = solver . integrate ( f , torch . linspace ( 0 , 20 , 201 ), history_function , None ) Please note that this library is fairly new so bugs might arise, if so please raise an issue !","title":"torchdde"},{"location":"#torchdde","text":"torchdde is a library providing Constant Lag Delay Differential Equations (DDEs) training neural networks via the adjoint method compatible with Pytorch .","title":"torchdde"},{"location":"#installation","text":"pip install git@github.com:usr/torchdde.git or locally git clone https://github.com/usr/torchdde.git cd torchdde/ pip install .","title":"Installation"},{"location":"#quick-example","text":"def f ( t , y , args , history ): return y * ( 1 - history [ 0 ]) delays = torch . tensor ([ 1.0 ]) solver = DDESolver ( RK2 (), delays ) history_values = torch . arange ( 1 , 5 ) . reshape ( - 1 , 1 ) history_function = lambda t : history_values solution , _ = solver . integrate ( f , torch . linspace ( 0 , 20 , 201 ), history_function , None ) Please note that this library is fairly new so bugs might arise, if so please raise an issue !","title":"Quick example"},{"location":"interpolators/linear-interpolator/","text":"This library only support linear interpolation at this moment ! torchdde.TorchLinearInterpolator \u00a4 Linear interpolator class that is compatible with batching. __init__ ( self , ts : Float [ Tensor , 'time' ], ys : Float [ Tensor , 'batch time ...' ]) \u00a4 Arguments: ts : Some increasing collection of times. ys : The observations themselves. add_point ( self , new_t : Float [ Tensor , '1' ], new_y : Float [ Tensor , 'batch ...' ]) \u00a4 Arguments: new_t : new timestamp added to the interpolation new_ys : new observation added to the interpolation","title":"Linear interpolator"},{"location":"interpolators/linear-interpolator/#torchdde.TorchLinearInterpolator","text":"Linear interpolator class that is compatible with batching.","title":"TorchLinearInterpolator"},{"location":"usage/faq/","text":"FAQ \u00a4 How would I define a DDE with several delays ? \u00a4 You just have to specify a delay tensor size that corresponds to the number of delays you desire. solver = .... delays = torch . tensor ([ 1.0 , 2.0 ]) history_function = lambda t : ... ts = ... def simple_dde ( t , y , args , * , history ): # this correspond to y'(t) = -y(t-1) - y(t-2) return - history [ 0 ] - history [ 1 ] dde_solver = DDESolver ( solver , delays ) ys , _ = dde_solver . integrate ( simple_dde , ts , history_function ) How about if I want a neural network to have also several delays ? \u00a4 Well if its the same forward pass in the Neural DDE , then nothing needs to be changed ! The term torch.cat([z, *history], dim=-1) unpacks all the delayed terms.","title":"FAQ"},{"location":"usage/faq/#faq","text":"","title":"FAQ"},{"location":"usage/faq/#how-would-i-define-a-dde-with-several-delays","text":"You just have to specify a delay tensor size that corresponds to the number of delays you desire. solver = .... delays = torch . tensor ([ 1.0 , 2.0 ]) history_function = lambda t : ... ts = ... def simple_dde ( t , y , args , * , history ): # this correspond to y'(t) = -y(t-1) - y(t-2) return - history [ 0 ] - history [ 1 ] dde_solver = DDESolver ( solver , delays ) ys , _ = dde_solver . integrate ( simple_dde , ts , history_function )","title":"How would I define a DDE with several delays ?"},{"location":"usage/faq/#how-about-if-i-want-a-neural-network-to-have-also-several-delays","text":"Well if its the same forward pass in the Neural DDE , then nothing needs to be changed ! The term torch.cat([z, *history], dim=-1) unpacks all the delayed terms.","title":"How about if I want a neural network to have also several delays ?"},{"location":"usage/getting-started/","text":"Getting Started \u00a4 An illustrative example which solves the following DDE \\(\\frac{dy}{dt}= -y(t-2), \\quad \\psi(t<0) = 2\\) over the interval \\([0, 5]\\) . import matplotlib.pyplot as plt import torch from torchdde import DDESolver , RK4 def simple_dde ( t , y , args , * , history ): return - history [ 0 ] device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) # Define the time span of the DDE ts = torch . linspace ( 0 , 5 , 51 ) ts = ts . to ( device ) # Define the delays, here there is only one tau=2.0 list_delays = torch . tensor ([ 2.0 ]) list_delays = list_delays . to ( device ) # Defining a constant history function for the DDE history_values = torch . tensor ([ 3.0 ]) history_values = history_values . reshape ( - 1 , 1 ) history_values = history_values . to ( device ) history_function = lambda t : history_values # Solve the DDE by using the RK4 method dde_solver = DDESolver ( RK4 (), list_delays ) ys , _ = dde_solver . integrate ( simple_dde , ts , history_function , None ) The numerical solver used is RK4 is the 4th Runge Kutta method. The solution is saved at each time stamp in ts . The initial step size is equal to ts[1]-ts[0] .","title":"Getting Started"},{"location":"usage/getting-started/#getting-started","text":"An illustrative example which solves the following DDE \\(\\frac{dy}{dt}= -y(t-2), \\quad \\psi(t<0) = 2\\) over the interval \\([0, 5]\\) . import matplotlib.pyplot as plt import torch from torchdde import DDESolver , RK4 def simple_dde ( t , y , args , * , history ): return - history [ 0 ] device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) # Define the time span of the DDE ts = torch . linspace ( 0 , 5 , 51 ) ts = ts . to ( device ) # Define the delays, here there is only one tau=2.0 list_delays = torch . tensor ([ 2.0 ]) list_delays = list_delays . to ( device ) # Defining a constant history function for the DDE history_values = torch . tensor ([ 3.0 ]) history_values = history_values . reshape ( - 1 , 1 ) history_values = history_values . to ( device ) history_function = lambda t : history_values # Solve the DDE by using the RK4 method dde_solver = DDESolver ( RK4 (), list_delays ) ys , _ = dde_solver . integrate ( simple_dde , ts , history_function , None ) The numerical solver used is RK4 is the 4th Runge Kutta method. The solution is saved at each time stamp in ts . The initial step size is equal to ts[1]-ts[0] .","title":"Getting Started"},{"location":"usage/neural-dde/","text":"Neural DDE \u00a4 Warning This library only supports constant lag DDEs. Therefore we are unable to model state dependent DDEs. This examples trains a Neural DDE to reproduce a simple dataset of a delay logistic equation. The backward pass is compute with the adjoint method i.e ddesolve_adjoint . import time import matplotlib.pyplot as plt import torch import torch.nn as nn from torch.utils.data import DataLoader , Dataset from torchdde import ddesolve_adjoint , DDESolver , Euler from torchvision.ops import MLP device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) Recalling that a neural DDE is defined as \\[\\frac{dy}{dt} = f_{\\theta}(t, y(t), y(t-\\tau_1), \\dots, y(t-\\tau_{n})), \\quad y(t<0) = \\psi(t)\\] then here we're now about to define \\(f_{\\theta}\\) that appears on that right hand side on the equation above class NDDE ( nn . Module ): def __init__ ( self , delays , in_size , out_size , width_size , depth , ): super () . __init__ () self . in_dim = in_size * ( 1 + len ( delays )) self . delays = torch . nn . Parameter ( delays ) self . mlp = MLP ( self . in_dim , hidden_channels = depth * [ width_size ] + [ out_size ], ) def forward ( self , t , z , args , * , history ): return self . mlp ( torch . cat ([ z , * history ], dim =- 1 )) We generate the toy dataset of the delayed logistic equation (Equation 2.1). def get_data ( y0 , ts , tau = torch . tensor ([ 1.0 ])): def f ( t , y , args , history ): return y * ( 1 - history [ 0 ]) solver = DDESolver ( Euler (), tau ) ys , _ = solver . integrate ( f , ts , lambda t : torch . unsqueeze ( y0 , dim = 1 ), None ) return ys class MyDataset ( Dataset ): def __init__ ( self , ys ): self . ys = ys def __getitem__ ( self , index ): return self . ys [ index ] def __len__ ( self ): return self . ys . shape [ 0 ] Main entry point. Try running main() . def main ( dataset_size = 128 , batch_size = 128 , lr = 0.0005 , max_epoch = 1000 , width_size = 32 , depth = 2 , seed = 5678 , plot = True , print_every = 5 , ): torch . manual_seed ( seed ) ts = torch . linspace ( 0 , 10 , 101 ) y0_min , y0_max = 2.0 , 3.0 y0 = ( y0_min - y0_max ) * torch . rand (( dataset_size ,)) + y0_max ys = get_data ( y0 , ts ) ts , ys = ts . to ( device ), ys . to ( device ) delay_min , delay_max = 0.7 , 1.3 value = ( delay_max - delay_min ) * torch . rand (( 1 ,)) + delay_min list_delays = torch . tensor ([ value ]) list_delays = list_delays . to ( device ) state_dim = ys . shape [ - 1 ] model = NDDE ( list_delays , state_dim , state_dim , width_size , depth ) model = model . to ( device ) dataset = MyDataset ( ys ) train_loader = DataLoader ( dataset , batch_size = batch_size , shuffle = True ) # Training loop like normal. model . train () loss_fn = torch . nn . MSELoss () optimizer = torch . optim . Adam ( model . parameters (), lr = lr ) for epoch in range ( max_epoch ): for step , data in enumerate ( train_loader ): t = time . time () optimizer . zero_grad () data = data . to ( device ) history_fn = lambda t : data [:, 0 ] ys_pred = ddesolve_adjoint ( history_fn , model , ts , None , Euler ()) loss = loss_fn ( ys_pred , data ) loss . backward () optimizer . step () if ( epoch % print_every ) == 0 or epoch == max_epoch - 1 : print ( \"Epoch : {} , Step {} / {} , Loss : {:.3e} , Tau {} , Time {} \" . format ( epoch , step + 1 , len ( train_loader ), loss . item (), [ d . item () for d in model . delays ], time . time () - t , ) ) if plot : plt . plot ( ts . cpu (), data [ 0 ] . cpu (), c = \"dodgerblue\" , label = \"Real\" ) history_values = data [ 0 , 0 ][ ... , None ] history_fn = lambda t : history_values ys_pred = ddesolve_adjoint ( history_fn , model , ts , Euler ()) plt . plot ( ts . cpu (), ys_pred [ 0 ] . cpu () . detach (), \"--\" , c = \"crimson\" , label = \"Model\" , ) plt . legend () plt . savefig ( \"neural_dde.png\" ) plt . show () plt . close () return ts , ys , model ts , ys , model = main ()","title":"Neural DDE"},{"location":"usage/neural-dde/#neural-dde","text":"Warning This library only supports constant lag DDEs. Therefore we are unable to model state dependent DDEs. This examples trains a Neural DDE to reproduce a simple dataset of a delay logistic equation. The backward pass is compute with the adjoint method i.e ddesolve_adjoint . import time import matplotlib.pyplot as plt import torch import torch.nn as nn from torch.utils.data import DataLoader , Dataset from torchdde import ddesolve_adjoint , DDESolver , Euler from torchvision.ops import MLP device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) Recalling that a neural DDE is defined as \\[\\frac{dy}{dt} = f_{\\theta}(t, y(t), y(t-\\tau_1), \\dots, y(t-\\tau_{n})), \\quad y(t<0) = \\psi(t)\\] then here we're now about to define \\(f_{\\theta}\\) that appears on that right hand side on the equation above class NDDE ( nn . Module ): def __init__ ( self , delays , in_size , out_size , width_size , depth , ): super () . __init__ () self . in_dim = in_size * ( 1 + len ( delays )) self . delays = torch . nn . Parameter ( delays ) self . mlp = MLP ( self . in_dim , hidden_channels = depth * [ width_size ] + [ out_size ], ) def forward ( self , t , z , args , * , history ): return self . mlp ( torch . cat ([ z , * history ], dim =- 1 )) We generate the toy dataset of the delayed logistic equation (Equation 2.1). def get_data ( y0 , ts , tau = torch . tensor ([ 1.0 ])): def f ( t , y , args , history ): return y * ( 1 - history [ 0 ]) solver = DDESolver ( Euler (), tau ) ys , _ = solver . integrate ( f , ts , lambda t : torch . unsqueeze ( y0 , dim = 1 ), None ) return ys class MyDataset ( Dataset ): def __init__ ( self , ys ): self . ys = ys def __getitem__ ( self , index ): return self . ys [ index ] def __len__ ( self ): return self . ys . shape [ 0 ] Main entry point. Try running main() . def main ( dataset_size = 128 , batch_size = 128 , lr = 0.0005 , max_epoch = 1000 , width_size = 32 , depth = 2 , seed = 5678 , plot = True , print_every = 5 , ): torch . manual_seed ( seed ) ts = torch . linspace ( 0 , 10 , 101 ) y0_min , y0_max = 2.0 , 3.0 y0 = ( y0_min - y0_max ) * torch . rand (( dataset_size ,)) + y0_max ys = get_data ( y0 , ts ) ts , ys = ts . to ( device ), ys . to ( device ) delay_min , delay_max = 0.7 , 1.3 value = ( delay_max - delay_min ) * torch . rand (( 1 ,)) + delay_min list_delays = torch . tensor ([ value ]) list_delays = list_delays . to ( device ) state_dim = ys . shape [ - 1 ] model = NDDE ( list_delays , state_dim , state_dim , width_size , depth ) model = model . to ( device ) dataset = MyDataset ( ys ) train_loader = DataLoader ( dataset , batch_size = batch_size , shuffle = True ) # Training loop like normal. model . train () loss_fn = torch . nn . MSELoss () optimizer = torch . optim . Adam ( model . parameters (), lr = lr ) for epoch in range ( max_epoch ): for step , data in enumerate ( train_loader ): t = time . time () optimizer . zero_grad () data = data . to ( device ) history_fn = lambda t : data [:, 0 ] ys_pred = ddesolve_adjoint ( history_fn , model , ts , None , Euler ()) loss = loss_fn ( ys_pred , data ) loss . backward () optimizer . step () if ( epoch % print_every ) == 0 or epoch == max_epoch - 1 : print ( \"Epoch : {} , Step {} / {} , Loss : {:.3e} , Tau {} , Time {} \" . format ( epoch , step + 1 , len ( train_loader ), loss . item (), [ d . item () for d in model . delays ], time . time () - t , ) ) if plot : plt . plot ( ts . cpu (), data [ 0 ] . cpu (), c = \"dodgerblue\" , label = \"Real\" ) history_values = data [ 0 , 0 ][ ... , None ] history_fn = lambda t : history_values ys_pred = ddesolve_adjoint ( history_fn , model , ts , Euler ()) plt . plot ( ts . cpu (), ys_pred [ 0 ] . cpu () . detach (), \"--\" , c = \"crimson\" , label = \"Model\" , ) plt . legend () plt . savefig ( \"neural_dde.png\" ) plt . show () plt . close () return ts , ys , model ts , ys , model = main ()","title":"Neural DDE"},{"location":"usage/solvers/","text":"Numerical Solvers \u00a4 ODE Solvers \u00a4 Only a few explicit solvers are available to use : Warning The following solver are constant step size solvers. This is indeed less flexible than adaptive stepsize method but such an incorporation isn't available at the time. torchdde.AbstractOdeSolver \u00a4 Base class for creating ODE solvers. All solvers should inherit from it. To create new solvers users must implement the step method. step ( self , func : Module , t : Float [ Tensor , '1' ], y : Float [ Tensor , 'batch ...' ], dt : Float [ Tensor , '1' ], args = None , has_aux = False ) -> Tensor abstractmethod \u00a4 ODE's stepping definition Arguments: func : Pytorch model, i.e vector field t : Current time step t y : Current state y dt : Stepsize dt has_aux : Whether the model has an auxiliary output. Returns: Integration result at time t+dt integrate ( self , func : Module , ts : Float [ Tensor , 'time' ], y0 : Float [ Tensor , 'batch ...' ], args = None , has_aux = False ) -> Tensor \u00a4 Integrate a system of ODEs. Arguments: func : Pytorch model, i.e vector field ts : Integration span y0 : Initial condition has_aux : Whether the model has an auxiliary output. Returns: Integration result over ts torchdde.Euler ( AbstractOdeSolver ) \u00a4 Euler's method torchdde.Ralston ( AbstractOdeSolver ) \u00a4 Ralston's method (2nd order) torchdde.RK2 ( AbstractOdeSolver ) \u00a4 2nd order explicit Runge-Kutta method torchdde.RK4 ( AbstractOdeSolver ) \u00a4 4th order explicit Runge-Kutta method DDE Solvers \u00a4 torchdde.DDESolver \u00a4 Solver class used to integrate a DDE with a given ODE solver. See torchdde.AbstractOdeSolver for more details on which solvers are available. __init__ ( self , solver : AbstractOdeSolver , delays : Float [ Tensor , 'delays' ]) \u00a4 Arguments: solver : Solver to integrate the DDE delays : Delays tensors used in DDE integrate ( self , func : Module , ts : Float [ Tensor , 'time' ], history_func : Callable , args ) -> Tuple [ Float [ Tensor , 'batch time ...' ], Callable ] \u00a4 Integrate a system of DDEs. Arguments: func : Pytorch model, i.e vector field ts : Integration span history_func : DDE's history function Returns: Integration result over ts and a TorchLinearInterpolator object of the result integration integrate_with_cubic_interpolator ( self , func : Module , ts : Float [ Tensor , 'time' ], history_func : Callable , args ) -> Tuple [ Float [ Tensor , 'batch time ...' ], Callable ] \u00a4 Integrate a system of DDEs. Please note that this method is not not efficient since NaturalCubicSpline needs to recompute its coefficients after each integration step. Please use [ torchdde.AbstractOdeSolver.integrate ] Arguments: func : Pytorch model, i.e vector field ts : Integration span history_func : DDE's history function Returns: Integration result over ts and a NaturalCubicSpline object of the result integration","title":"Numerical Solvers"},{"location":"usage/solvers/#numerical-solvers","text":"","title":"Numerical Solvers"},{"location":"usage/solvers/#ode-solvers","text":"Only a few explicit solvers are available to use : Warning The following solver are constant step size solvers. This is indeed less flexible than adaptive stepsize method but such an incorporation isn't available at the time.","title":"ODE Solvers"},{"location":"usage/solvers/#torchdde.AbstractOdeSolver","text":"Base class for creating ODE solvers. All solvers should inherit from it. To create new solvers users must implement the step method.","title":"AbstractOdeSolver"},{"location":"usage/solvers/#torchdde.Euler","text":"Euler's method","title":"Euler"},{"location":"usage/solvers/#torchdde.Ralston","text":"Ralston's method (2nd order)","title":"Ralston"},{"location":"usage/solvers/#torchdde.RK2","text":"2nd order explicit Runge-Kutta method","title":"RK2"},{"location":"usage/solvers/#torchdde.RK4","text":"4th order explicit Runge-Kutta method","title":"RK4"},{"location":"usage/solvers/#dde-solvers","text":"","title":"DDE Solvers"},{"location":"usage/solvers/#torchdde.DDESolver","text":"Solver class used to integrate a DDE with a given ODE solver. See torchdde.AbstractOdeSolver for more details on which solvers are available.","title":"DDESolver"},{"location":"usage/training-dde/","text":"Training a DDE \u00a4 Two following ways are possible to train Neural DDE : optimize-then-discretize discretize-then-optimize Please see the doctorial thesis On Neural Differential Equations for more information on both procedures. optimize-then-discretize \u00a4 If you choose to train with the adjoint method then you only to use ddesolve_adjoint : import torch from torchdde import ddesolve_adjoint history_function = lambda t : ... ts = torch . linspace ( ... ) pred = ddesolve_adjoint ( history_function , model , ts , args , solver ) torchdde . ddesolve_adjoint ( history_func : Callable , func : Module , ts : Float [ Tensor , 'time' ], args , solver : AbstractOdeSolver ) -> Float [ Tensor , 'batch time ...' ] \u00a4 Main function to integrate a constant time delay DDE with the adjoint method Arguments: history_func : DDE's history function func : Pytorch model, i.e vector field ts : Integration span solver : ODE solver use Returns: Integration result over ts . discretize-then-optimize \u00a4 Warning You are unable to learn the DDE's delays if using the discretize-then-optimize approach. If you choose to train with the inherent auto differentiation capabilities of Pytorch then you need to use DDESolver with specified : history function history_function , pytorch model model , integration span ts , ode solver used solver . import torch from torchdde import ddesolve_adjoint ode_solver = ... tensor_delays = ... dde_solver = DDEsolver ( ode_solver , tensor_delays ) history_function = lambda t : ... ts = torch . linspace ( ... ) pred , _ = dde_solver . integrate ( model , ts , history_function , args )","title":"Training a DDE"},{"location":"usage/training-dde/#training-a-dde","text":"Two following ways are possible to train Neural DDE : optimize-then-discretize discretize-then-optimize Please see the doctorial thesis On Neural Differential Equations for more information on both procedures.","title":"Training a DDE"},{"location":"usage/training-dde/#optimize-then-discretize","text":"If you choose to train with the adjoint method then you only to use ddesolve_adjoint : import torch from torchdde import ddesolve_adjoint history_function = lambda t : ... ts = torch . linspace ( ... ) pred = ddesolve_adjoint ( history_function , model , ts , args , solver )","title":"optimize-then-discretize"},{"location":"usage/training-dde/#torchdde.ddesolve_adjoint","text":"Main function to integrate a constant time delay DDE with the adjoint method Arguments: history_func : DDE's history function func : Pytorch model, i.e vector field ts : Integration span solver : ODE solver use Returns: Integration result over ts .","title":"ddesolve_adjoint()"},{"location":"usage/training-dde/#discretize-then-optimize","text":"Warning You are unable to learn the DDE's delays if using the discretize-then-optimize approach. If you choose to train with the inherent auto differentiation capabilities of Pytorch then you need to use DDESolver with specified : history function history_function , pytorch model model , integration span ts , ode solver used solver . import torch from torchdde import ddesolve_adjoint ode_solver = ... tensor_delays = ... dde_solver = DDEsolver ( ode_solver , tensor_delays ) history_function = lambda t : ... ts = torch . linspace ( ... ) pred , _ = dde_solver . integrate ( model , ts , history_function , args )","title":"discretize-then-optimize"},{"location":"usage/training-ode/","text":"Training a ODE \u00a4 First, there are a lot of available package to use to train Neural ODEs, torchdiffeq (not maintained anymore) in Pytorch and Diffrax . This means that this library doesn't have any many features since it focuses more on DDEs. Two following ways are possible to train Neural ODE : optimize-then-discretize discretize-then-optimize Please see the doctorial thesis On Neural Differential Equations for more information on both procedures. optimize-then-discretize \u00a4 If you choose to train with the adjoint method then you only to use odesolve_adjoint : import torch from torchdde import odesolve_adjoint history_function = torch . Tensor ([ ... ]) ts = torch . linspace ( ... ) pred = odesolve_adjoint ( y0 , model , ts , args , solver ) torchdde . odesolve_adjoint ( z0 : Float [ Tensor , 'batch ...' ], func : Module , ts : Float [ Tensor , 'time ...' ], args , solver : AbstractOdeSolver ) -> Float [ Tensor , 'batch time ...' ] \u00a4 discretize-then-optimize \u00a4 If you choose to train with the inherent auto differentiation capabilities of Pytorch then you need to use AbstractOdeSolver with specified : initial condition y0 , pytorch model model and integration span ts . import torch from torchdde import odesolve_adjoint ode_solver = ... y0 = ... ts = torch . linspace ( ... ) pred = ode_solver . integrate ( model , ts , y0 , args )","title":"Training a ODE"},{"location":"usage/training-ode/#training-a-ode","text":"First, there are a lot of available package to use to train Neural ODEs, torchdiffeq (not maintained anymore) in Pytorch and Diffrax . This means that this library doesn't have any many features since it focuses more on DDEs. Two following ways are possible to train Neural ODE : optimize-then-discretize discretize-then-optimize Please see the doctorial thesis On Neural Differential Equations for more information on both procedures.","title":"Training a ODE"},{"location":"usage/training-ode/#optimize-then-discretize","text":"If you choose to train with the adjoint method then you only to use odesolve_adjoint : import torch from torchdde import odesolve_adjoint history_function = torch . Tensor ([ ... ]) ts = torch . linspace ( ... ) pred = odesolve_adjoint ( y0 , model , ts , args , solver )","title":"optimize-then-discretize"},{"location":"usage/training-ode/#torchdde.odesolve_adjoint","text":"","title":"odesolve_adjoint()"},{"location":"usage/training-ode/#discretize-then-optimize","text":"If you choose to train with the inherent auto differentiation capabilities of Pytorch then you need to use AbstractOdeSolver with specified : initial condition y0 , pytorch model model and integration span ts . import torch from torchdde import odesolve_adjoint ode_solver = ... y0 = ... ts = torch . linspace ( ... ) pred = ode_solver . integrate ( model , ts , y0 , args )","title":"discretize-then-optimize"}]}